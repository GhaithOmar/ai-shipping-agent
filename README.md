# AI Shipping Agent ðŸš€

A production-leaning AI Agent project built step by step.  
This repo will grow into a full **LLM-powered support assistant** using **LangChain/LangGraph**, **FastAPI**, **Docker**, and a lightly fine-tuned LLM (LoRA).

---

## âœ… Day 1 â€” FastAPI Skeleton

Today we set up the project foundation:

- Created a **FastAPI app** with two endpoints:
  - `GET /health` â†’ simple health check.
  - `POST /chat` â†’ stub endpoint that echoes back the userâ€™s message.
- Installed core dependencies (`fastapi`, `uvicorn`, `pydantic`, `python-dotenv`).
- Created a **virtual environment** for isolated development.
- Saved dependencies into `backend/requirements.txt`.

---

## ðŸ”§ Quickstart (local)

### 1. Clone repo & enter folder
```bash
git clone <your-repo-url>
cd ai-shipping-agent
```

### 2. Create & activate virtual environment (PowerShell)
```powershell
python -m venv .venv
.venv\Scripts\Activate.ps1
```

### 3. Install dependencies
```bash
pip install -r backend/requirements.txt
```

### 4. Run the FastAPI server
```bash
uvicorn backend.main:app --reload
```

### 5. Test it
Open [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

- Try **GET /health** â†’ returns:
  ```json
  {"status": "ok"}
  ```

- Try **POST /chat** with:
  ```json
  { "message": "hello world" }
  ```
  â†’ returns:
  ```json
  { "answer": "Echo: hello world", "citations": [] }
  ```

---

## ðŸ“‚ Project Structure (Day 1)

```text
ai-shipping-agent/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## ðŸš€ Next Steps (Day 2 Preview)

- Collect 20â€“40 FAQ pages (DHL, Aramex, UPS, etc.) into `rag/data/`.
- Build an ingestion script to **chunk â†’ embed â†’ store in Qdrant**.
- Add `/search` endpoint to test retrieval.

---

## ðŸ“š Day 2 â€” Ingestion & Search

Today we added the first Retrieval-Augmented Generation (RAG) layer:

- Wrote ingestion pipeline (`rag/ingest.py`):
  - Loads `.md` files from `rag/data/` (skips `README.md`).
  - Splits into overlapping character chunks.
  - Embeds with **BAAI/bge-m3** and stores in local **Qdrant**.
- Added `/search` endpoint â†’ validates retrieval end-to-end.
- Indexed **synthetic shipping FAQs & policies** (14 chunks).

### Notes
- The dataset is **synthetic**, generated with the help of ChatGPT.  
- Uses placeholder carriers (*Shipping Company A/B/C*) to avoid trademark/legal issues.  
- Purpose: demo MVP pipeline with realistic structure, but no real data.

## Day 3 â€” Tiny LoRA (Production Path)

**Data (v0.2):**
- Sources: Bitext Customer Support (HF), Customer Support on Twitter (Kaggle).
- Processing: PII scrub, carrier aliasing (Shipping_A/B/C), de-twitterization (strip @handles, URLs, ^tags, â€œDM usâ€), near-dup dedup, shipping-intent filter.
- Rebalanced ratios: Bitext 85%, Kaggle 5%, Synthetic 10%.
- Format: JSONL with {"input", "assistant_response"} (â‰¤1024 tokens).

**Training:**
- Base: Llama 3.1 8B Instruct (4-bit).
- LoRA: r=8, alpha=16, dropout=0.1, targets: attention only (q/k/v/o).
- Trainer: TRL SFT (legacy API) with HF DataCollatorForLanguageModeling.
- Output: adapter-only saved under /MyDrive/ai_shipping_agent/lora-llama3.1-8b.

**Eval:**
- System prefix enforces: ask-before-answer, no links, concise steps, defer facts to RAG.
- Decoding guardrails: blocked URL/handles/tags, conservative max_new_tokens.
- Result: clean tone without Twitter artifacts; still requests IDs and avoids live-claiming.

## ðŸ§ª Day 4 â€” Fine-tune & Inference Integration

**Data v0.2 (20k examples):**
- Built new supervised set with ~20,000 pairs (17,600 train + 2,400 val).
- Auto-split logged by the builder and stored to `data/v0.2/` (`mini_sft.jsonl`, `mini_sft_val.jsonl`).

**Training:**
- LoRA/PEFT SFT using TRL on Colab L4 (24GB), 1 epoch to reduce risk of overfitting.
- Best run saved; eval loss â‰ˆ **0.77**. Adapters pushed to the Hugging Face Hub (user: **GhaithOmar**).

**Inference / Guardrails:**
- Implemented `infer_guarded` with deterministic decoding, link suppression, and intent-aware â€œmissing IDâ€ nudge.
- Added a small rule set to scrub social artifacts and enforce concise bullets.

**Backend wiring:**
- Created `backend/generation.py` and updated `backend/main.py` to load base + adapter on startup and expose `/chat` with RAG context.
- Added `tests/smoke/smoke.py` to sanity-check loading, search, and a sample round trip.

**Artifacts to check in:**
- `configs/data_prep_v02.yaml` (metadata bump to v0.2).
- `data/v0.2/mini_sft.jsonl` and `mini_sft_val.jsonl`.
- `backend/generation.py`, updated `backend/main.py`.
- `tests/smoke/smoke.py`.

---

## ðŸ“‚ Project Structure (Day 4)
```text
ai-shipping-agent/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ generation.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ search.py
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ data_prep_v01.yaml
â”‚   â””â”€â”€ data_prep_v02.yaml
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ v0.1/
â”‚   â”‚   â”œâ”€â”€ manifest.json
â”‚   â”‚   â”œâ”€â”€ mini_sft.jsonl
â”‚   â”‚   â””â”€â”€ mini_sft_val.jsonl
â”‚   â””â”€â”€ v0.2/
â”‚       â”œâ”€â”€ manifest.json
â”‚       â”œâ”€â”€ mini_sft.jsonl
â”‚       â””â”€â”€ mini_sft_val.jsonl
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ Day3/Tiny_LoRA_Smoke-Train.ipynb
â”‚   â””â”€â”€ Day4/Full_LoRA_Train.ipynb
â”œâ”€â”€ qdrant_db/
â”‚   â””â”€â”€ collection/shipping_kb/storage.sqlite
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ ingest.py
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ common_policies.md
â”‚       â”œâ”€â”€ company_a_faq.md
â”‚       â”œâ”€â”€ company_b_faq.md
â”‚       â”œâ”€â”€ company_c_faq.md
â”‚       â”œâ”€â”€ tracking_status_reference.md
â”‚       â””â”€â”€ README.md
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ data_prep/build_sft.py
â””â”€â”€ tests/
    â””â”€â”€ smoke/smoke.py
```

## Day 5 â€” Agent Integration (LangChain + LangGraph)

**What we added**
- `backend/tools/`:
  - `search_kb.py`: Qdrant retriever (prefers embedded `qdrant_db/`, softâ€‘fails to `[]` if unavailable) + `format_citations`.
  - `parse_tracking.py`: regex extractor for carrier aliases + tracking IDs.
  - `estimate_eta.py`: rough ETA rules (business days) with GCC/EU region tweaks.
- `backend/agent/`:
  - `graph.py`: minimal LangGraph app with nodes:
    `history_read â†’ understand â†’ tool_router â†’ respond â†’ history_write`.
    Reuses the existing `infer_guarded` for final text.
  - `memory.py`: inâ€‘process shortâ€‘term memory (last ~6 turns).
- `backend/main.py`:
  - Agent toggle: `POST /chat?agent=1` uses the LangGraph path; default `POST /chat` keeps legacy RAG path.
  - Streaming: `POST /chat/stream` streams the agentâ€™s answer lineâ€‘byâ€‘line.
- Tests:
  - `tests/agent/test_tools.py`, `tests/agent/test_api_agent.py` + smoke tests; pass even without a model/Qdrant server.

```powershell
uvicorn backend.main:app --reload
# Agent path (JSON)
Invoke-RestMethod -Method Post -Uri "http://127.0.0.1:8000/chat?agent=1" `
  -ContentType "application/json" -Body '{"message":"When will my package arrive JO to AE express?"}'

# Legacy search
Invoke-RestMethod -Method Post -Uri "http://127.0.0.1:8000/search" `
  -ContentType "application/json" -Body '{"query":"change address","k":3}'

# Streaming (arrives as a single string in PowerShell)
$body = '{"message":"Track order 12345 with Shipping_A and give last two scans."}'
(Invoke-WebRequest -Method Post -Uri "http://127.0.0.1:8000/chat/stream" `
  -ContentType "application/json" -Body $body).Content
```

ðŸ“‚ Project Structure (Day 5 â€” after Agent Integration)
```text
ai-shipping-agent/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # FastAPI app; /chat (legacy + agent toggle), /chat/stream
â”‚   â”œâ”€â”€ generation.py        # Guarded inference (LoRA adapter + safety rules)
â”‚   â”œâ”€â”€ settings.py          # Pydantic settings loader (.env support)
â”‚   â”œâ”€â”€ search.py            # Legacy RAG search helper (kept for compatibility)
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â”œâ”€â”€ graph.py         # LangGraph state machine (understand â†’ tool_router â†’ respond + memory)
â”‚   â”‚   â””â”€â”€ memory.py        # Short-term in-process memory (last 6 turns)
â”‚   â””â”€â”€ tools/
â”‚       â”œâ”€â”€ search_kb.py     # Qdrant semantic search retriever + citation formatter
â”‚       â”œâ”€â”€ parse_tracking.py# Regex parser for carrier + tracking IDs
â”‚       â””â”€â”€ estimate_eta.py  # Simple ETA rules (business-day estimates)
â”œâ”€â”€ rag/
â”‚   â””â”€â”€ ingest.py            # Ingestion pipeline â†’ builds qdrant_db/ from markdown data
â”œâ”€â”€ qdrant_db/               # Embedded Qdrant collection (local persistence)
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ agent/               # Unit tests (tools + API agent branch)
â”‚   â””â”€â”€ smoke/               # Smoke tests (model, RAG, agent path)
â”œâ”€â”€ configs/                 # Dataset prep configs (v0.1, v0.2 YAMLs)
â”œâ”€â”€ data/                    # Training/eval datasets (v0.1, v0.2 JSONL)
â”œâ”€â”€ notebooks/               # Colab notebooks (LoRA training runs)
â”œâ”€â”€ scripts/                 # Data prep scripts
â”œâ”€â”€ README.md
â””â”€â”€ .env.example             # Example environment variables
**Run (PowerShell)**

```
## ðŸ§­ Day 6 â€” Agent polish, new tool, streaming & citations

**What we shipped today**

- **Agent as the smooth default**: `/chat` now uses the LangGraph path (toggle with `AGENT_ENABLE` or `?agent=0` to force legacy).
- **New tool â€” `rate_quote`**: quick, explainable label price estimate with zone/service rules.
- **Stronger `parse_tracking`**: accepts hyphenated / spaced / mixed IDs, normalizes to canonical form.
- **Citations normalized**: both paths return `["source#chunk", ...]` (e.g., `company_a_faq.md#12`).  
- **Streaming**:
  - `/chat/stream` (agent): chunked SSE (client contract identical to token streams).
  - `/chat/stream?agent=0` (legacy): **token SSE** when a model is loaded, **offline chunked** fallback otherwise.
- **Resilience**: all endpoints degrade gracefully even if no model/GPU is available.
- **Reâ€‘ingest**: Qdrant payload now includes `source` (filename) and `chunk_id` (perâ€‘file index) for traceable citations.

---

## ðŸ“‚ Project Structure (Day 6)

```text
ai-shipping-agent/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                 # FastAPI app: /health, /chat, /chat/stream (SSE)
â”‚   â”œâ”€â”€ generation.py           # Guarded inference + token streaming helper
â”‚   â”œâ”€â”€ settings.py             # Env-driven config (AGENT_ENABLE, QDRANT_*)
â”‚   â”œâ”€â”€ search.py               # Legacy retriever (returns source + chunk_id)
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â”œâ”€â”€ graph.py            # LangGraph: understand â†’ tool_router â†’ respond (+ memory)
â”‚   â”‚   â””â”€â”€ memory.py           # Short-term memory (last N turns)
â”‚   â””â”€â”€ tools/
â”‚       â”œâ”€â”€ search_kb.py        # Qdrant retriever; emits KBHit with source + chunk_id
â”‚       â”œâ”€â”€ parse_tracking.py   # Robust tracking parser (hyphens/spaces â†’ normalized)
â”‚       â”œâ”€â”€ estimate_eta.py     # Handbook ETA rules (business days, regions)
â”‚       â””â”€â”€ rate_quote.py       # NEW: handbook label price estimate
â”œâ”€â”€ rag/
â”‚   â””â”€â”€ ingest.py               # Markdown â†’ chunks â†’ BGE-M3 â†’ Qdrant (writes source + chunk_id)
â”œâ”€â”€ qdrant_db/                  # Embedded Qdrant store (created by ingest)
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ agent/                  # Unit tests (graph/tools)
â”‚   â””â”€â”€ smoke/                  # Smoke tests (API/e2e)
â”œâ”€â”€ .env.example
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Environment

Create `.env` (or copy from `.env.example`) and adjust if needed:

```env
# Toggle default path (true = agent by default)
AGENT_ENABLE=true

# Optional model settings (leave empty if no GPU; app will degrade gracefully)
BASE_MODEL=
ADAPTER_ID=
FALLBACK_BASE=Qwen/Qwen2.5-3B-Instruct
HUGGINGFACE_TOKEN=

# Qdrant (embedded)
QDRANT_PATH=qdrant_db
QDRANT_COLLECTION=shipping_kb
```

> **Tip:** If you donâ€™t have a capable GPU, keep model variables empty. The agent and legacy paths still return helpful, handbookâ€‘based replies with citations.

---

## ðŸ“¦ Ingest the Knowledge Base

```powershell
# PowerShell
Remove-Item -Recurse -Force .\qdrant_db\ -ErrorAction SilentlyContinue
python .\rag\ingest.py
```

```bash
# Bash
rm -rf qdrant_db
python rag/ingest.py
```

Quick sanity check (optional):
```powershell
python - << 'PY'
from backend.search import search
print([ (h["source"], h.get("chunk_id")) for h in search("return policy", k=3) ])
PY
```

---

## â–¶ï¸ Run the API

```powershell
# PowerShell
uvicorn backend.main:app --reload --port 8000
```

```bash
# Bash
uvicorn backend.main:app --reload --port 8000
```

Health:
```powershell
Invoke-RestMethod -Uri "http://127.0.0.1:8000/health" -Method Get
```
```bash
curl -s http://127.0.0.1:8000/health
```

---

## ðŸ§ª Quick Chat Tests

**Agent (default)**
```powershell
$body = @{ message = "Track order 12345678 with Shipping_A"; top_k = 2 } | ConvertTo-Json
Invoke-RestMethod -Uri "http://127.0.0.1:8000/chat" -Method Post -ContentType "application/json" -Body $body
```

```bash
curl -s -X POST "http://127.0.0.1:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{"message":"Track order 12345678 with Shipping_A","top_k":2}'
```

**Force legacy**
```powershell
$body = @{ message = "Return policy for fragile items"; top_k = 3 } | ConvertTo-Json
Invoke-RestMethod -Uri "http://127.0.0.1:8000/chat?agent=0" -Method Post -ContentType "application/json" -Body $body
```

```bash
curl -s -X POST "http://127.0.0.1:8000/chat?agent=0" \
  -H "Content-Type: application/json" \
  -d '{"message":"Return policy for fragile items","top_k":3}'
```

You should see a JSON response like:
```json
{
  "answer": "- ...",
  "citations": ["company_a_faq.md#6", "company_b_faq.md#9"]
}
```

---

## ðŸ“¡ Streaming (SSE)

**Agent (chunked SSE output)**
```powershell
$body = @{ message = "Track order 12345678 with Shipping_A"; top_k = 2 } | ConvertTo-Json
$r = Invoke-WebRequest -Uri "http://127.0.0.1:8000/chat/stream" -Method Post -ContentType "application/json" -Body $body
$r.Content
```

```bash
curl -N -X POST "http://127.0.0.1:8000/chat/stream" \
  -H "Content-Type: application/json" \
  -d '{"message":"Track order 12345678 with Shipping_A","top_k":2}'
```

**Legacy (token SSE if model is loaded, otherwise offline-chunked SSE)**
```powershell
$body = @{ message = "Return policy for fragile items"; top_k = 3 } | ConvertTo-Json
$r = Invoke-WebRequest -Uri "http://127.0.0.1:8000/chat/stream?agent=0" -Method Post -ContentType "application/json" -Body $body
$r.Content
```

```bash
curl -N -X POST "http://127.0.0.1:8000/chat/stream?agent=0" \
  -H "Content-Type: application/json" \
  -d '{"message":"Return policy for fragile items","top_k":3}'
```

---

## ðŸ§° Tests

```powershell
# Unit tests
python -m pytest -q tests/agent

# Full suite
python -m pytest -q

# (If API is running, the embedded DB may be locked on Windows; either stop Uvicorn
# or isolate a test store)
$env:QDRANT_PATH = "qdrant_db_test"
Remove-Item -Recurse -Force .\qdrant_db_test -ErrorAction SilentlyContinue
python -m pytest -q
Remove-Item -Recurse -Force .\qdrant_db_test -ErrorAction SilentlyContinue
```

```bash
# Bash
python -m pytest -q
```

---

## ðŸ§¯ Troubleshooting

- **Empty citations on agent path** â†’ Reâ€‘ingest with the updated payload (must contain `source` + `chunk_id`).  
- **â€œModel not loadedâ€ in legacy streaming** â†’ expected without GPU; endpoint streams offline chunks instead of failing.  
- **PowerShell quoting issues** â†’ prefer `Invoke-RestMethod` or hereâ€‘strings for JSON; avoid multiâ€‘line `curl.exe` unless you doubleâ€‘escape.  
- **Weird characters (â€“, â†’)** â†’ console encoding; server normalizes to ASCII in responses.

---

## ðŸ”’ Limitations (Day 6)

- **Data realism**: KB docs are synthetic and for demo only; citations are precise (`source#chunk`) but not from real carrier content.  
- **Model hosting**: LoRA adapters are not deployed by default; legacy token streaming requires a local/remote model.  
- **Evaluation**: smoke tests verify routing/guardrails; no largeâ€‘scale RAGAS or human eval yet.  
- **Streaming in agent path**: chunked SSE today (clientâ€‘compatible); full token streaming through the graph can be added later if needed.


## ðŸ”® Limitations & Future Work

This project was built as a **portfolio showcase** to demonstrate the end-to-end design of an AI shipping support agent. While it integrates retrieval, fine-tuned generation, and agent orchestration, there are some known limitations and clear paths for future work.

### ðŸ“‰ Current Limitations
- **Data realism**  
  - FAQ knowledge base content is **synthetic**, generated to resemble real carrier FAQs and policies.  
  - Fine-tuning data combines public **Customer Support Twitter (Kaggle)** and **Bitext datasets**, with preprocessing for PII scrub + carrier aliasing.  
  - These datasets approximate real support conversations but are **not production carrier data**.

- **Model deployment**  
  - The LoRA-fine-tuned **Llama-3.1-8B Instruct** is tested successfully offline (Colab / local GPU).  
  - Due to GPU cost constraints, the LoRA adapter is **not hosted live**. Online demos may instead route to a base model API (e.g., Together/Fireworks) while preserving the pipeline.

- **Evaluation**  
  - Current testing relies on smoke tests and qualitative inspection.  
  - No large-scale **RAGAS** or structured human evaluation has been performed yet.

### ðŸš€ Future Work
- **Data improvements** â†’ Incorporate real carrier datasets (FAQs, scan events, policies) for higher realism.  
- **Scalable deployment** â†’ Host fine-tuned model + retriever via GPU cloud infra (AWS/GCP/RunPod) with Docker/K8s.  
- **Evaluation framework** â†’ Add automated metrics (context precision/recall, answer faithfulness) and human eval.  
- **Additional tools** â†’ Extend agent with rate quote, pickup scheduling, and customs clearance calculators.  
- **Monitoring & guardrails** â†’ Add observability, tracing, and stronger safety filters for production reliability.  

---

## ðŸ“Œ Project Status

This repository represents a **research prototype and learning project**, not a production-ready system.  
It demonstrates the following end-to-end skills:  
- Data curation & preprocessing (synthetic + public datasets).  
- LoRA fine-tuning on Llama-3.1-8B with guardrails.  
- RAG integration with Qdrant + semantic search.  
- Agent orchestration with LangGraph and tool-calling.  
- FastAPI backend + Docker packaging.  

âš ï¸ **Disclaimer:** Since the project uses synthetic and public datasets, and the fine-tuned model is not hosted live, results should be considered illustrative only.  
